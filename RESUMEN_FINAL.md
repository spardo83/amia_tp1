# TP 2: LDA/QDA y optimizaci√≥n matem√°tica - RESUMEN FINAL

    
    ## Integrantes
    
    - Pardo, Sebasti√°n
    - Gonz√°lez, Mart√≠n
    - Braz√≥n, Josmar
    - Losada, Ricardo
  


##  Implementaciones Completadas

Todas las implementaciones requeridas han sido completadas y funcionan correctamente:

### 1. **Tensorizaci√≥n** (Preguntas 1-7)
-  `QDA`: Implementaci√≥n est√°ndar
-  `TensorizedQDA`: Paraleliza sobre clases
-  `FasterQDA`: Elimina ciclo for en predicci√≥n
-  `EfficientQDA`: Usa propiedad matem√°tica para evitar matriz n√ón

### 2. **Factorizaci√≥n de Cholesky** (Preguntas 8-13)
-  `QDA_Chol1`: Usa `LA.inv(cholesky(...))`
-  `QDA_Chol2`: Usa `solve_triangular` (m√°s eficiente)
-  `QDA_Chol3`: Usa `dtrtri` (LAPACK)
-  `TensorizedChol`: Combina tensorizaci√≥n con Cholesky
-  `EfficientChol`: Implementaci√≥n m√°s optimizada

##  Resultados de Testing

Todas las implementaciones mantienen la misma precisi√≥n:
- **Accuracy**: 0.981 (98.1%)
- **Dataset**: Wine (178 observaciones, 13 features, 3 clases)
- **Split**: 70% train, 30% test

## üîß Archivos del Proyecto

### Implementaciones
- `implementations_final.py`: Todas las implementaciones corregidas y funcionales
- `TP1_original.ipynb`: Notebook original del TP

### Testing y Debug
- `test_final.py`: Script de prueba final
- `debug_faster_qda.py`: Debug espec√≠fico para FasterQDA
- `debug_test.py`: Debug general de implementaciones

### Documentaci√≥n
- `respuestas_teoricas.md`: Respuestas a todas las preguntas te√≥ricas
- `README.md`: Instrucciones de uso
- `requirements.txt`: Dependencias

## Respuestas a Preguntas Clave

### Pregunta 1: ¬øSobre qu√© paraleliza TensorizedQDA?
**Respuesta**: TensorizedQDA paraleliza sobre las **k clases**, no sobre las n observaciones. Usa operaciones tensoriales para calcular la forma cuadr√°tica para todas las clases simult√°neamente.

### Pregunta 4: ¬øD√≥nde aparece la matriz n√ón?
**Respuesta**: En `FasterQDA`, la matriz n√ón aparece en el producto `unbiased_x.T @ inv_cov @ unbiased_x` cuando `unbiased_x` tiene shape `(p, n)`. Esta matriz contiene todas las interacciones entre observaciones.

### Pregunta 5: Demostraci√≥n matem√°tica
**Respuesta**: Se demuestra que `diag(A¬∑B) = sum(A ‚äô B^T, axis=1)`, donde ‚äô es la multiplicaci√≥n elemento por elemento. Esto permite calcular solo la diagonal sin construir la matriz completa n√ón.

### Pregunta 8: A^(-1) en t√©rminos de L
**Respuesta**: Si A = LL^T, entonces A^(-1) = L^(-T) ¬∑ L^(-1). Esto es √∫til porque:
`(x-Œº)^T Œ£^(-1) (x-Œº) = ||L^(-1)(x-Œº)||^2`

### Pregunta 9: Diferencias entre implementaciones Cholesky
**Respuesta**:
- **Chol1**: Calcula L^(-1) expl√≠citamente
- **Chol2**: Usa `solve_triangular` (m√°s eficiente)
- **Chol3**: Usa LAPACK para L^(-1)

## C√≥mo Usar

### 1. Instalar dependencias
```bash
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt
```

### 2. Probar implementaciones
```bash
python test_final.py
```

### 3. Usar en c√≥digo
```python
from implementations_final import *

# Cargar datos
X_full, y_full = get_wine_dataset()
y_full_encoded = label_encode(y_full)

# Probar implementaci√≥n
model = EfficientQDA()
model.fit(X_train, y_train)
predictions = model.predict(X_test)
```

## Optimizaciones Implementadas

### 1. **Tensorizaci√≥n**
- Usar `np.stack()` para paralelizar sobre clases
- Operaciones tensoriales para calcular m√∫ltiples clases simult√°neamente

### 2. **Eliminaci√≥n de Bucles**
- Vectorizar operaciones cuando sea posible
- Reducir iteraciones innecesarias

### 3. **Propiedades Matem√°ticas**
- Usar `diag(A¬∑B) = sum(A ‚äô B^T, axis=1)` para evitar matrices n√ón
- Calcular solo la diagonal sin construir la matriz completa

### 4. **Factorizaci√≥n de Cholesky**
- Usar `solve_triangular` en lugar de inversi√≥n
- Aprovechar la estructura triangular de L

## Conceptos Clave Aprendidos

1. **Clasificaci√≥n Bayesiana**: Entender la regla de decisi√≥n de Bayes
2. **Forma Cuadr√°tica**: Optimizar el c√°lculo de `(x-Œº)^T Œ£^(-1) (x-Œº)`
3. **Tensorizaci√≥n**: Paralelizar operaciones usando tensores
4. **Factorizaci√≥n de Cholesky**: Usar LL^T para optimizar inversiones
5. **Optimizaci√≥n Num√©rica**: Balancear precisi√≥n, velocidad y memoria

## An√°lisis de Performance

Bas√°ndome en la teor√≠a y las implementaciones:

1. **Tensorizaci√≥n**: TensorizedQDA deber√≠a ser m√°s r√°pido en predicci√≥n
2. **Optimizaci√≥n**: EfficientQDA deber√≠a ser m√°s eficiente en memoria
3. **Cholesky**: Chol2 deber√≠a ser el m√°s eficiente por usar `solve_triangular`
4. **Combinaciones**: EfficientChol deber√≠a ser la implementaci√≥n m√°s optimizada

## Versiones Utilizadas

- **Python**: 3.13.5
- **NumPy**: 2.3.2
- **SciPy**: 1.16.1
- **scikit-learn**: 1.7.1
- **pandas**: 2.3.1
- **tqdm**: 4.67.1

